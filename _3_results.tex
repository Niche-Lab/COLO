\section{Results and Discussion}

\subsection*{Public Dataset: COLO}

The COLO dataset is organized in YOLO and COCO formats and published on the online platforms GitHub \href{https://github.com/Niche-Squad/COLO/}{(https://github.com/Niche-Squad/COLO/)} and Huggingface \href{https://huggingface.co/datasets/Niche-Squad/COLO}{(https://huggingface.co/datasets/Niche-Squad/COLO)}. The dataset consists of eight configurations:
\textit{0\_all}, \textit{1\_top}, \textit{2\_side}, \textit{3\_external}, \textit{a1\_t2s}, \textit{a2\_s2t}, \textit{b\_light}, and \textit{c\_external}. The \textit{0\_all} configuration serves as the baseline for this study, featuring non-overlapping training and testing images collected from both the Top-View Camera and Side-View Camera. The \textit{1\_top}, \textit{2\_side}, and \textit{3\_external} configurations contain images from their respective cameras. The \textit{a1\_t2s}, \textit{a2\_s2t}, and \textit{b\_light} configurations include training/testing splits for the Top2Side, Side2Top, and Day2Night scenarios, respectively. The \textit{c\_external} configuration features training images from the Top-View and Side-View Cameras, with testing images from the External Camera. The dataset hosted on GitHub is available as a compressed zip file for public access. In contrast, the dataset on Huggingface requires the Python package "datasets" \cite{datasets} to download. The Huggingface version offers additional functionality to resize the images and annotations to specific resolutions, providing greater flexibility for various applications.

\subsection*{Evaluation Metrics}

The model performance for each combination of training configuration, model architecture, and training sample size was measured using four metrics: $\text{mAP@{0.5:0.95}}$, $\text{mAP@{0.5}}$, precision, and recall. A pair-wise comparison of these metrics is presented in Figure \ref{fig:metrics} to illustrate their interrelationships. The $\text{mAP@{0.5:0.95}}$ metric is the most stringent, requiring the model to achieve both high positioning accuracy (i.e., high IoU) and high precision. In contrast, $\text{mAP@{0.5}}$ is more lenient, requiring only high confidence but moderate IoU in the predictions. It was observed that when $\text{mAP@{0.5:0.95}}$ exceeds 0.5, $\text{mAP@{0.5}}$ typically converges to 0.95. Similar trends were noted for precision and recall, which are metrics focusing on moderate-confidence and IoU thresholds. These trends suggest that $\text{mAP@{0.5:0.95}}$ is a more reliable indicator of model performance, as it is less likely to be "saturated" by high-performing predictions. When counting cows is the primary objective and precise positioning is less critical, an mAP@0.5 value of 0.9 is sufficient. This level of performance can be achieved with smaller models and limited training data. For example, our results showed that the YOLOv8n model, trained on 32 samples, achieved an mAP@0.5 of 0.9, making it suitable for such applications.
Another key observation is the relationship between precision and recall. Generally in this study, higher precision is associated with higher recall; however, there are instances where models exhibit high recall but low precision. This phenomenon is particularly evident in the Side2Top and External configurations when the sample size is smaller than 64. This indicates a tendency for the model to misclassify non-cow objects as cows more frequently than it misses detecting cows. This finding is crucial for the practical application of the model, as it suggests a tendency to overestimate rather than underestimate the number of cows in the images.
It is also important to note that mAP@0.5:0.95 is the most stringent metric, and achieving an accuracy greater than 0.90 on this metric is generally unrealistic. Typically, a value of 0.7 is considered good and is sufficient to yield a precision and recall of around 0.9.

In summary, the Evaluation Metrics section provides a comprehensive framework for assessing the performance of the YOLO models. These metrics offer insights into both the localization and classification capabilities of the models, helping to identify strengths and weaknesses under different environmental conditions and camera angles. The observations emphasize that for applications where counting cows is more critical than precise positioning, achieving a high mAP@0.5 is adequate, while the stringent mAP@0.5:0.95 metric serves as a robust indicator of overall model performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Figure/figure_s1.jpg}
    \caption{Pairwise correlation among different evaluation metrics.}
    \label{fig:metrics}
\end{figure}


%%%%%%%%%%%%% Rewritten the section by Mautushi%%%%%%%%%%%%%%%%
\section{Evaluation Metrics}

To assess the performance of the YOLO models, we used four key metrics: mAP@0.5:0.95, mAP@0.5, precision, and recall. These metrics provide a comprehensive understanding of how well the models detect and localize cows in the images from the COLO dataset. A pair-wise comparison of these metrics is presented in Figure \ref{fig:metrics} to illustrate their interrelationships.

The mAP@0.5:0.95 metric is the most stringent, requiring the model to achieve both high positioning accuracy and high precision across IoU thresholds from 0.5 to 0.95. Because it is less likely to be influenced by high-confidence predictions alone, it serves as a reliable indicator of overall model performance. However, achieving an accuracy greater than 0.90 on this metric is generally unrealistic. Typically, a value of 0.7 is considered good and is sufficient to yield precision and recall of around 0.9.

In contrast, mAP@0.5 is more lenient, requiring high confidence but only moderate IoU. It measures the average precision at an IoU threshold of 0.5. For applications where counting cows is more important than precise positioning, an mAP@0.5 value of 0.9 is sufficient. For example, our results showed that the YOLOv8n model, trained on 32 samples, achieved an mAP@0.5 of 0.9, making it suitable for such applications.

Precision and recall metrics focus on the accuracy and completeness of the detections. Precision is the ratio of true positive detections to the total number of positive detections (true positives + false positives), measuring how accurate the positive predictions are. Recall is the ratio of true positive detections to the total number of actual positives (true positives + false negatives), measuring the modelâ€™s ability to detect all relevant objects. Generally, higher precision is associated with higher recall. However, in some configurations, such as Side2Top and External with smaller sample sizes, models exhibited high recall but low precision. This indicates a tendency to misclassify non-cow objects as cows more frequently than missing actual cows, suggesting a tendency to overestimate rather than underestimate the number of cows in the images.

These metrics were used to evaluate the performance of the models under different environmental conditions and camera angles. Our observations, illustrated in Figure 3, highlight that mAP@0.5:0.95 is a robust indicator of model performance, while mAP@0.5 is sufficient for applications focused on counting rather than precise positioning.

In summary, the Evaluation Metrics section outlines a robust framework for assessing the performance of the YOLO models. These metrics provide insights into both the localization and classification capabilities of the models, helping to identify strengths and weaknesses under different environmental conditions and camera angles. For applications where counting cows is more critical than precise positioning, achieving a high mAP@0.5 is adequate, while the stringent mAP@0.5:0.95 metric serves as a comprehensive indicator of overall model performance.





\subsection*{Study 1: The changes in camera view angles dramatically affect the model performance}




%%%%%%%%%%%%%%%%% written by me %%%%%%%%%%%%%%%%%%%%%
The baseline training configuration showed good generalization capability, with over 90\% of the predictions correctly positioning cows at the 50\% IoU criterion ($\text{mAP@{0.5}}$). Further, the generalization performance can be dissected into changes in view angles (i.e., Top2Side and Side2Top) and lighting conditions (i.e., Day2Night). Changes in lighting conditions did not dramatically affect model performance across all four metrics. However, changing camera views resulted in a performance drop of approximately 30\% and 60\% in $\text{mAP@{0.5}}$ for the Side2Top and Top2Side configurations, respectively. Across all metrics and training sample sizes, the Top2Side configuration consistently showed the worst performance.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Figure/figure_3.jpg}
    \caption{The performance of YOLOv9e across various data configurations and training sample sizes. In the upper left and right corners, the plots display the metrics mAP50:95 and mAP50, respectively, for different training samples across diverse data configurations. The lower left and right plots depict precision and recall values, again for varying training samples and configurations.}
    \label{fig:schemes}
    \end{figure}
From the perspective of precision and recall, changing the camera view from Top2Side resulted in the model missing more than 7 out of every 10 cows, with only 50\% of the detection being correct. For the 'External' configuration, our model is identifying 6 cows out of every 10 cows, which is not ideal but also not the worst performance observed. Notably, performance in the Day2Night configuration was close to the baseline in terms of precision, which only considers predictions with high confidence compared to $\text{mAP@{0.5}}$. Hence, by excluding low-confidence predictions, changes in lighting conditions did not affect model performance. Regardless of the configuration and evaluation metrics, model performance always increased as the training sample sizes increased.
%%%%%%%%%%% Discussion of study 1 %%%%%
%\subsubsection*{Discussion for Study 1: Potential causes for the performance drop in camera view points}

This study  provides a comparative analysis of the behavior of the model in various data configurations. It is clear that the `Day2Night' configuration shown a much better performance relative to the heterogeneous viewpoint-oriented configurations `Top2Side' and `Side2Top'.

Despite the various challenges in adapting models from day to night conditions, the `Day2Night' configuration consistently maintains high precision, closely mirroring the `Baseline' configuration across all training sample sizes. This suggests that changes in lighting have less impact on the models ability to detect objects compared to changes in viewpoint. This robustness to lighting could be attributed to the inclusion of diverse lighting conditions in the training phase, specifically, model performance benefited from pixel-wise augmentation techniques such as adjustments to hue, saturation, and value(HSV). These augmentations introduced a variety of color variations to the images, enhancing the model's ability to generalize across different visual conditions.Such methods adjust the lighting and contrast in training images, enhancing the model's ability to generalize to different illumination scenarios. Moreover, these YOLO models benefit from pre-training on the COCO dataset, which is characterized by a wide array of images with varied lighting, aiding their adaptability to shifts in light.

On the other hand, the models perform suboptimally in scenarios involving changes in viewpoint. Each new viewpoint introduces fundamentally different object features that are not replicated through standard data augmentation methods such as lighting or affine transformations. For example, when the camera is placed at a lower angle, cows are more frequently occluded by stalls and fences. These additional objects introduce variations that cannot be mitigated by augmentations in HSV space or image translation. Consequently, 'Top2Side' performs the worst, as it is particularly challenging to identify cows from the side. Even for the 'External' configuration, the model struggles to generalize well despite being trained on the 'Baseline' configuration because the camera angle is changed again in the 'External' setup. In summary, camera view angle is crucial for model generalization, with side views being the most challenging.




\subsection*{Study 2: A Higher Model Complexity Does Not Always Lead to Better Performance}

\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{Figure/figure_4.jpg}
\caption{The performance of YOLOv8 and YOLOv9 models across various model parameters and data configurations, evaluated using four metrics: mAP 50:95, mAP 50, precision, and recall. Each column indicates a different data configuration, starting from top left to bottom right: Baseline', Day2Night', Side2Top', Top2Side', and `External'. The horizontal axis of all plots indicates the number of model parameters.}
\label{fig:models}
\end{figure}



Our study found that the training configuration significantly affects the relationship between model complexity and performance. Based on Study 1, predicting images from a side view using a model trained on top-view camera images is one of the most challenging tasks. In this configuration, increasing model complexity generally resulted in poorer generalization, with simpler models often performing better. However, in other configurations that demonstrated better generalization in Study 1, the peak performance was not always achieved by the most complex model. For example, in the baseline configuration, the YOLOv9e model performed best in terms of $\text{mAP@{0.5:0.95}}$, $\text{mAP@{0.5}}$, and recall, while the YOLOv8m model excelled in precision. Neither of these models had the highest parameter counts compared to YOLOv8x. It is also worth noting that different model architectures showed different performance trends with varying complexities. The YOLOv8-family models tended to perform best with mid-sized models (i.e., YOLOv8m), whereas larger models in the YOLOv9 family usually performed better. Hence, the study concluded that model performance is determined by both the training configuration and the model architecture.


%%%%%%%%%%% discussion for study2 %%%%%%%%%%%%%%%%%%
Our analysis, as shown in Figure \ref{fig:models}, indicates that although both YOLOv8 \cite{ultralyticsYOLOv8} and YOLOv9 \cite{wang2024yolov9} models exhibit an increase in $\text{mAP@{0.5}}$ with more parameters when trained on the COCO dataset \cite{lin2014microsoft}, this does not support a definitive conclusion that more parameters consistently improve model performance. This may be because the prior work's findings were based on the COCO dataset, which includes 80 classes and mainly features standalone images. In contrast, our study uses an indoor farm dataset focused exclusively on a single class: cows. Consequently, the model may not need as many parameters to effectively detect cows. This suggests that researchers should not rely solely on public dataset performance, as model generalization is specific to the task and dataset.

Additionally, our study found that a small model such as YOLOv8, with only 3.2M parameters, can yield 90\% accuracy with a relatively small size of training samples. This indicates that when one encounters a simple and homogenous task like positioning cows, deploying a small model is optimal in balancing computing time and prediction accuracy. This further underscores the importance of considering the specific characteristics of the task and dataset when choosing a model, rather than defaulting to more complex models under the assumption they will perform better.

Overall, our findings emphasize that higher model complexity does not necessarily lead to better performance. The optimal model configuration depends heavily on the specific task and dataset, highlighting the need for careful model selection tailored to the particular application at hand.



\subsection*{Study 3: The advantages of custom initial weight  is limited when the model is simple}


The results presented in Figure \ref{fig:finetune} indicate that the benefit of using fine-tuned initial weights is minimal for simpler models. Specifically, when employing YOLOv8n, the performance difference between the default and fine-tuned weights was insignificant when fine-tuning data from the Top-View Camera and Side-View Camera. However, as model complexity increased, a greater number of fine-tuning samples were required for the two different initial weights to achieve similar performance. For instance, in the case of YOLOv9e, the performance gap was eliminated when the number of fine-tuning samples reached 128 and 64 for the Top-View Camera and Side-View Camera data sources, respectively. A similar trend was observed with the External camera, where a significant performance gap of more than 25\% in $\text{mAP@{0.5:0.95}}$ was observed for YOLOv9e when the sample size was 16. It is also noted that, although the performance gap was closed to zero for the Top-View Camera and Side-View Camera data sources, the gap was never closed for the External camera. 
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Figure/figure_5.jpg}
    \caption{Variation in mAP50:95 across different models, sample sizes, and weight initialization conditions. Green lines represent instances where weights were initialized with fine-tuned models (cases 3 and 4, as detailed in Table ~\ref{tab:configuration}), while orange lines indicate scenarios employing default weights (trained with the COCO dataset). The horizontal axis indicates the number of training samples used for fine-tuning.}
    \label{fig:finetune}
\end{figure}

%%%%%%%%%%%%% discussion for study 3 %%%%%%%%%%%%

The analysis from our study goal 3 suggest that, for YOLO models with fewer parameters, such as YOLOv8n and YOLOv8m, the choice of weight initialization does not make a significant difference in fine-tuning performance. In contrast, larger models like YOLOv8x, YOLOv9c, and YOLOv9e exhibit improved performance when weights are initialized from a model that has been previously fine-tuned, as highlighted in cases 2 and 4 in Table \ref{tab:configuration}.

Therefore, when fine-tuning larger models with a limited dataset, it is beneficial to utilize weights previously fine-tuned on various data configurations. This approach leverages the additional learned features and adaptability from the initial fine-tuning, resulting in better performance even with a small amount of new data. For example, our results showed that YOLOv9e achieved optimal performance with fewer fine-tuning samples when initialized with fine-tuned weights compared to default weights.

Conversely, for smaller models, the weight initialization strategy does not significantly impact fine-tuning performance. This is likely due to the lower complexity and fewer parameters of these models, which makes them less dependent on the initial weight configuration to achieve good performance. In practical terms, this means that for simpler models, researchers can save time and computational resources by directly fine-tuning without the need for customized weight initialization.

The analysis of Figure ~\ref{fig:finetune} also provides another insight on the performance across homogeneous viewpoint data configurations, specifically `Top-view camera' and `Side-view camera'. The data demonstrates that the `Top-view camera' configuration consistently yields higher mAP values regardless of the training sample size and weight initialization conditions. This implies that the `Side-view camera' configuration, where both training and test images are captured from the side view, presents a more formidable challenge for cow detection compared to the `Top-view camera' configuration. The side view poses difficulties due to occlusions by neighboring cows and additional distractions, such as obstacles in aisles and fences. Furthermore, cows located further away in side-view images may not be as visible, complicating feature extraction. In contrast, the `Top-view camera' configuration benefits from an unobstructed aerial perspective, ensuring that the top view of all cow instances is clearly visible and free from such obstructions. This distinction in visibility between the two configurations contributes to the ease of feature extraction and ultimately, the performance disparity observed. These findings align with the results from Study 1, which demonstrated that changes in camera view angles dramatically affect model performance. In Study 1, we found that models trained on top-view datasets struggled the most to detect cows from side-view images, with performance dropping by approximately 60\% in mAP@0.5. This significant drop in performance is attributed to the same reasons identified in Study 3: the side view introduces occlusions and distractions that are not present in the top view, making feature extraction more challenging.

Moreover, when working with external or unseen datasets, fine-tuning with custom initial weights which is trained on relevant tasks brings advantages to the detection tasks.  On the other hand, simpler models do not benefit much from customized weights, suggesting that it is more efficient to train a simple model with pre-trained weights without relying on prior relevant information, which sometimes requires intensive labor efforts.

In summary, the study highlights the importance of custom weight initialization for fine-tuning larger models, especially when data is limited. Using fine-tuned weights can greatly improve model performance, making it an important factor when deploying complex models in environments with limited resources. On the other hand, simpler models do not gain as much from this strategy, which allows for easier and more efficient fine-tuning processes.





\section*{Evaluation of Computational Resource Requirements}
\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth]{figure_6.jpg}
\caption{Comparative evaluation of computational resource requirements. (a) Training time (expressed as a multiple of baseline time) versus number of parameters for YOLOv8 and YOLOv9 models, presented for training sample sizes of 32 (left), 128 (middle), and 500 (right). (b) Inference frequency versus number of parameters for YOLOv8 and YOLOv9 models on GPU (left) and CPU (right). (c) A table displaying the weight sizes and parameter counts of various YOLOv8 and YOLOv9 models.}

\label{fig:resources}
\end{figure}
The required computational resources were evaluated in terms of training time (Figure ~\ref{fig:resources}a), inference time (Figure ~\ref{fig:resources}b), and model weight sizes (Figure ~\ref{fig:resources}c). Training time was presented as a ratio of the actual training time over the baseline, which is the time required to train the YOLOv8n model with only 32 samples. The results suggested that using the largest model, YOLOv8x, with 20 times more parameters, increased training time by 4 to 6 times depending on the training sample size. Additionally, the YOLOv9 models generally required more training time and had slower inference frames per second (FPS) compared to the YOLOv8 models. The gap in training time expanded as the number of training samples increased. Inference time was calculated as the average FPS in a batch of 64 images. Running the models on CPU with the smallest model (i.e., YOLOv8n) was found to be slower than running the largest model (i.e., YOLOv8x) on GPU, where the FPS were 19.77 and 29.21, respectively. The importance of high FPS models was highlighted in this study, as real-time inference usually requires a model with an FPS higher than 30. Implementing these YOLO models on CPU may not meet this requirement according to the results.

%%%%%%%%%%%%%%%%% rewritten section by mautushi %%%%%%%%%%%%%%%%%%%
\section{Evaluation of Computational Resource Requirements}

The evaluation of computational resource requirements is crucial for understanding the feasibility of deploying YOLO models in real-world applications, especially in environments with limited computational resources. This section compares training time (Figure ~\ref{fig:resources}a), inference time (Figure ~\ref{fig:resources}b), and model weight sizes (Figure ~\ref{fig:resources}c) sizes for various YOLO models.

The training time for each model was measured and expressed as a multiple of the baseline training time, which is the time required to train the YOLOv8n model with 32 samples. The results indicated that using the largest model, YOLOv8x, which has 20 times more parameters, increased training time by 4 to 6 times, depending on the training sample size. Additionally, the YOLOv9 models generally required more training time and had slower inference frames per second (FPS) compared to the YOLOv8 models. The gap in training time expanded as the number of training samples increased.

Inference time was measured as the average FPS in a batch of 64 images. Running the models on a CPU with the smallest model (YOLOv8n) was slower than running the largest model (YOLOv8x) on a GPU. Specifically, the FPS for YOLOv8n on a CPU was 19.77, while the FPS for YOLOv8x on a GPU was 29.21. High FPS models are essential for real-time inference, which usually requires a model with an FPS higher than 30. The results indicate that implementing these YOLO models on a CPU may not meet this requirement.

To provide a more concrete comparison of the hardware used in this study, the Apple M1 Max GPU with 32 cores achieves approximately 10.4 TFLOPS of FP32 performance. This is comparable to the NVIDIA GeForce RTX 2080, which has around 10.1 TFLOPS of FP32 performance \cite{notebookcheck_m1_max}. On the other hand, the NVIDIA A100 80GB GPU delivers approximately 19.5 TFLOPS of FP32 performance \cite{nvidia_a100}.

Model weight sizes were also considered, impacting memory requirements and deployment feasibility, especially in edge computing environments. The weight sizes and parameter counts of various YOLO models are displayed in ~\ref{fig:resources}).

Briefly, the evaluation of computational resource requirements highlights the trade-offs between model complexity and computational efficiency. The larger YOLO models, while offering potentially better performance, require significantly more computational resources. This analysis helps researchers and practitioners select the appropriate model based on the available computational resources and the specific requirements of their application.



\section{Conclusion}

This study examined the impact of various training configurations and model complexities on the performance of YOLOv8 and YOLOv9 models for cow detection in indoor farm environments. Our results show that model performance is highly dependent on camera viewpoints, with side views presenting the greatest challenges. Additionally, fine-tuning models with weights from similar datasets substantially enhances performance, particularly for complex models in scenarios with limited data. We also introduce a public cow localization dataset, 'COLO', to support the research community.

The findings indicate that while increasing model complexity can improve performance, this is not always the case, especially in challenging configurations like 'Top2Side'. Models trained on a single viewpoint exhibit limited generalization, highlighting the importance of incorporating diverse and consistent camera angles in the training data.

Despite the promising results, this study has certain limitations. The models' performance was evaluated under specific indoor farm conditions, which may not generalize to all livestock environments. Moreover, the reliance on pre-defined configurations may limit the applicability of our findings to more dynamic settings.

Future work should explore adaptive methods for enhancing model generalization across varied viewpoints and environmental conditions. Additionally, investigating the integration of advanced data augmentation techniques and more diverse datasets could further improve detection accuracy and robustness.

In conclusion, this study offers valuable insights into optimizing object detection models for precision livestock farming, identifies critical factors influencing model performance, and provides the public 'COLO' dataset to facilitate further research and advancements in the field.