\section{Materials and Methods}

\subsection*{Cow Husbandry}

All procedures involving cow handling and image capturing were conducted in accordance with ethical guidelines and approved by the Virginia Tech Institutional Animal Care and Use Committee (IACUC \#22-146). The cows studied were part of the dairy herd at the Virginia Tech Dairy Complex in Blacksburg, Virginia, USA, which comprises approximately ~80\% Holstein and ~20\% Jersey cows. For the 'External' setting, the study included  100\% Holstein cows. The milking cows were housed in pens within a free-stall barn, featuring two rows of sand-bedded stalls, headlocks at the feed bunk, and two water troughs per pen. The stocking density was maintained at 100\% (i.e., one cow per stall). Heat stress was managed using automatic 48-inch diameter fans positioned over the stalls and feeding alleys. Cows were milked twice daily at 1:00 am and 12:00 pm in a double-twelve parallel milking parlor. They were fed ad libitum (with less than 5\% refusals) once daily at 8:00 am with a total mixed ration (TMR) consisting of approximately 42\% corn silage, 8\% grass hay, and 50\% concentrate on a dry matter basis. Manure from the stalls was removed at each milking session by personnel driving the cows to milking. Manure from the walking alleys within the pen was cleared two or three times daily using an automatic flushing system with recycled water. Fresh or recycled sand was added on a weekly basis.

\subsection*{Image Dataset}

The images in this study were collected using the Amazon Ring camera model Spotlight Cam Battery Pro (Ring Inc.), which offers a real-time video feed of dairy cows. Three cameras were installed in the barn: two at a height of 3.25 meters (10.66 feet) above the ground covering an area of 33.04 square meters (355.67 square feet). One camera provided a top view while the other was angled approximately 40 degrees from the horizontal to offer a side view of the cows. These are hereafter referred to as \textit{the top-view camera} and \textit{the side-view camera}, respectively. A third camera, termed \textit{the external camera}, was set at a lower height of 2.74 meters (9.00 feet) and covered a larger area of 77.63 square meters (835.56 square feet). Positioned 10 degrees downward from the horizontal, it captured a challenging perspective prone to occlusions among cows.

Images were captured using an Ring Application Programming Interface (API) \citep{greif_dgreifring_2024}, configured to record a ten-second video clip every 30 minutes continuously for 14 days. Since the image quality relies on the camera's internet connection, which was occasionally unstable, some images were found to be tearing or unrecognizable. Hence, the resulting dataset was manually curated for consistent quality, comprising 504 images from \textit{the top-view camera}, 500 from \textit{the side-view camera}, and 250 from \textit{the external camera}. These images were further categorized based on the lighting conditions: for \textit{the top-view camera}, 296 images were captured during daylight, 118 in the evening under artificial lighting, and 90 as near-infrared images without artificial light. From \textit{the side-view camera}, 113 images were taken in the evening, and 97 as near-infrared images. All images from \textit{the external camera} were captured during the day. Image examples are shown in \textbf{Figure~\ref{fig:dataset}a}.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figure_1.jpg}
    \caption{\textbf{Overview of the COLO dataset}. 1a. Seven instance images from the dataset with red bounding boxes labeling the location of cows. The columns show three different view angles: top-view, side-view, and external. The rows show three different lighting conditions: daylight, indoor, and near-infrared. 1b. An example of the annotated image in YOLO format. W, H, $w_b$, and $h_b$ represent the width, height, width of the bounding box, and height of the bounding box, respectively. x and y represent the center coordinates of the bounding box.} 
    \label{fig:dataset}
\end{figure}

The image annotations were conducted using an online platform, Roboflow \cite{roboflow2023}, to define cow positions in the images. The bounding boxes were manually drawn to enclose the cow contours, providing the coordinates of the top-left corners and the width and height of the boxes. If cows were partially occluded, the invisible parts were inferred based on the adjacent visible parts. If the cow position was too far from the camera, making important body features such as the head, tail, and legs unrecognizable, the cow was excluded from the dataset. The final annotations were saved in the YOLO format \cite{ultralytics2023datasets}, where annotations were stored in a text file with one row per cow in the image, each row containing the cow's class, center coordinates, width, and height of the bounding box. The graphical representation of the annotated images is shown in \textbf{Figure~\ref{fig:dataset}b}.

\subsection*{Model Training}

The model training was implemented using the Python library Ultralytics \cite{ultralytics}. The model hyperparameters were set to the default values in the library. The training epochs were set to 100, and the batch size was set to 16. The implemented data augmentation included randomly changing the image color hue, saturation, and exposure to improve the model's generalization to different lighting conditions. Geometry augmentation was also applied by randomly flipping the images horizontally, copying and pasting to mix up object instances across multiple images to increase data diversity, and randomly scaling the images to simulate different distances between the camera and the cows. The details of the hyperparameters are shown in \textbf{Table~\ref{tab:hyperparameters}}. The training was conducted on an NVIDIA A100 GPU (NVIDIA, USA) with 80GB video memory provided by Advanced Research Computing at Virginia Tech.

\subsection*{Model Evaluation}

The examined YOLO models are object detection models that return positions of detected objects (i.e., cows in this study) for the evaluated images. The detections are represented by a list of bounding boxes. Regardless of specific procedures among YOLO variants for computational efficiency, such as YOLOv8, which integrates objectness scores and conditional class probabilities into a single confidence score, each detection generally consists of $4+c$ elements: the xy-coordinates, width, and height of the bounding box, and the $c$ confidence scores indicating the probability of the object belonging to each of the $c$ classes. The class with the highest confidence score is considered the predicted class of the object. To evaluate the model performance, two aspects are considered: the localization accuracy and the classification accuracy. The localization accuracy is measured by the Intersection over Union (IoU) between the predicted bounding box and the ground truth bounding box. On the other hand, the classification accuracy is measured by the precision and recall given the confidence threshold. If the confidence score of a detection is higher than the threshold, the detection is considered a positive detection. Otherwise, the detection is neglected. Combining the localization and classification accuracy, the mean Average Precision ($mAP$) averages the area under the precision-recall curve across all the classes. The curve is generated by varying the confidence threshold from 0 to 1 given an IoU threshold. In this study, four metrics were used in the evaluation: the precision and recall at the confidence threshold of 0.25 and IoU threshold of 0.5, the mAP at the IoU threshold of 0.5 (noted as $\text{mAP@{0.5}}$), and the averaged mAP at varying IoU thresholds ranging from 0.5 to 0.95 (noted as $\text{mAP@{0.5:0.95}}$).

\subsection*{Study 1: Benchmarking Model Generalization Across Different Environmental Conditions}

To compare the performance drop between different view angles and lighting conditions, we designed a cross-validation strategy where models were trained on one dataset configuration and tested on another. There are five training configurations in this study (\textbf{Figure~\ref{fig:splits}}):

\begin{itemize}
    \item \textbf{Baseline:} The model was trained and evaluated on the dataset characterized for all conditions, including top-view, side-view, daylight, evening, and near-infrared images. The images did not overlap between the training and evaluation sets.
    \item \textbf{Top2Side:} The model was trained on the top-view images and evaluated on the side-view images.
    \item \textbf{Side2Top:} The model was trained on the side-view images and evaluated on the top-view images.
    \item \textbf{Day2Night:} The model was trained on the daylight images and evaluated on the evening images, including both artificial lighting and near-infrared images.
    \item \textbf{External:} The model was trained on images collected by the top-view and side-view cameras and evaluated on the external camera images.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figure_2.jpg}
    \caption{\textbf{Cross-validation configurations}. The training and testing sets were split into five different configurations: Baseline, Top2Side, Side2Top, Day2Night, and External.}
    \label{fig:splits}
\end{figure}

To study how the training sample size affects model performance in each configuration, the testing set in the cross-validation was fixed to the same 100 images. Then, the training set size was iteratively altered from 16 to 512 images, with the sample size doubled at each step. Each training sample size was repeated 50 times with different random seeds to ensure the robustness of the results. The YOLOv9e, which is the most capable model in the YOLO family to date according to its performance on the COCO dataset, was used as the base model for this study.

\subsection*{Study 2: The Correlation Between Model Complexity and Performance on the Tasks of Localizing Cows}

To investigate whether model performance increases with model complexity, five YOLO-family models were examined in this study. Three of the models were selected from the YOLOv8 family: YOLOv8n, YOLOv8m, and YOLOv8x. All YOLOv8 models share a similar architecture, differing in their depth multiplier, width multiplier, and ratio factor, which collectively determine their parameter counts of 3.2 million (m), 25.9m, and 68.2m, respectively. The depth multiplier determines how many convolutional layers are repeated in a C2F module, the novelty of YOLOv8. The width multiplier and ratio factor collectively specify the channel numbers in the convolutional operations. Correspondingly, YOLOv8n, YOLOv8m, and YOLOv8x are defined by depth multipliers of 0.33, 0.67, and 1.0, respectively. The width multipliers are 0.25, 0.75, and 1.25, while the ratio factors are 2.0, 1.5, and 1.0 \cite{v8yaml}. These variations enable the models to achieve different balances between computational efficiency and accuracy.

The remaining two models were YOLOv9c and YOLOv9e, the latest models in the YOLO family, with parameter counts of 25.6M and 58.2M, respectively. Unlike YOLOv8 models, these models have slightly different backbone architectures. Although the majority of the components between YOLOv9c and YOLOv9e are the same, they primarily differ in their layer counts, module complexities, and depth configurations. YOLOv9c has 618 layers and uses simpler modules, resulting in a more efficient model with lower computational demands. Conversely, YOLOv9e has 1225 layers and employs more advanced modules \cite{v9yaml}. All models were trained on 500 images in each of the four configurations described in Study 1.

In addition to model performance, computing speed was also evaluated. The training speed was recorded in seconds per 100 epochs on NVIDIA A100 GPUs (NVIDIA, USA), and the inference time was recorded as frames per second (FPS) on both the CPU and GPU (Apple M1 Max chip, Apple Inc., USA). The relationship between model complexity and time consumption was analyzed to provide insights into the trade-off between model performance and computational cost.

\subsection*{Study 3: Assessing the Advantages of Using Fine-Tuned Model Over the Pre-Trained Model as Initial Model Weights}

Most models are released with pre-trained weights obtained from large datasets containing millions of object instances (e.g., COCO \cite{lin2014microsoft} and ImageNet \cite{deng2009imagenet}). The pre-trained models have a general capability in recognizing common objects such as vehicles, animals, and household items. When the model is required to recognize specific objects (i.e., cows in this study), a model trained on a smaller but specific dataset is expected to have better performance. However, such advantages may not necessarily persist as the training sample size increases. Having an equally large number of samples for both the pre-trained and fine-tuned models could diminish the performance gap between the two models. To investigate this hypothesis, this study evaluated the performance of fine-tuned models with two different initial weights. The first initial weight was the default weight from the pre-trained model on the COCO dataset, while the second initial weight was the weight from the fine-tuned model on the opposite view angle. The cross-validation settings are described in \textbf{Table~\ref{tab:configuration}}.

\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Finetuning and Prediction Task} & \textbf{Initial Weights} \\
        \midrule
        Top-View Camera & COCO (pre-trained) \\
        & Side-View Camera (fine-tuned) \\
        \midrule
        Side-View Camera & COCO (pre-trained) \\
        & Top-View Camera (fine-tuned) \\
        \midrule
        External Camera & COCO (pre-trained) \\
        & Top-View and Side-View Cameras (fine-tuned) \\
        \bottomrule
    \end{tabular}
    \vspace{3mm}
    \caption{Finetuning configurations with different initial weights}
    \label{tab:configuration}
\end{table}

The backbones of all models (i.e., YOLOv8n, YOLOv8m, YOLOv8x, YOLOv9c, and YOLOv9e) were fine-tuned across different training sample sizes: 16, 32, 64, 128, 256, and 500. The goal was to determine whether the advantage of using the fine-tuned weights persists under the interaction between model complexity and different fine-tuning samples. The performance of the models was evaluated using $\text{mAP@{0.5:0.95}}$.


