\section{Introduction}

\subsection*{Object Detection and Its Applications}

Localizing livestock individuals from images or videos has become an essential task in precision livestock farming (PLF) \cite{fernandes2020image}. Such techniques allow farm operators to manage animal well-being and health in real-time, optimizing their resource management and improving sustainability \cite{morrone2022industry, hao2023cattle}. Technically speaking, in the field of computer vision (CV), which is a subfield of artificial intelligence (AI) that focuses on translating visual information into actionable insights, localization tasks can be further categorized into object detection, object segmentation, and pose estimation. Object detection is the simplest form among these tasks, localizing objects of interest by enclosing them within a rectangular bounding box defined by x and y coordinates, pixel width, and pixel height \cite{viola2001rapid}. Successful instances in this category include YOLO (You Only Look Once) \cite{redmon2016you}, Faster R-CNN (Region Convolutional Neural Networks) \cite{girshick2015fast}, and SSD (Single Shot MultiBox Detector) \cite{liu2016ssd}. These models have been adopted and applied by animal scientists for detection in precision livestock farming. For example, a study \cite{yu2022automatic} leveraged the DRN-YOLO model \cite{xu2020improved} to predict the eating behavior of dairy cows. This approach automates the assessment of feeding behavior, a critical indicator of cow health and productivity, and has saved labor efforts in complex farm settings. Another notable work is presented in \cite{nasirahmadi2019deep}, where the authors developed a posture detection system for pigs using deep learning models such as Faster R-CNN, SSD, and R-FCN, coupled with 2D imaging. This system accurately identifies standing and lying postures of pigs under commercial farm settings.

To achieve finer localization, object segmentation is employed to outline object contours pixel-wise, while pose estimation is performed by orienting and marking the key points of the object \cite{hariharan2015hypercolumns}. Some popular object segmentation models include Mask R-CNN \cite{he2017mask}, MS R-CNN \cite{huang2019mask}, and U-Net \cite{siddique2021u}. This method of segmentation has also been applied in the field of PLF. In the study \cite{noe2022automatic}, the authors developed a method using Mask R-CNN \cite{he2017mask} to segment and outline cattle in feedlots. Their technique enhances images and extracts key frames to accurately detect cattle, achieving superior precision with a mean pixel accuracy of 0.92. This supports advanced, real-time monitoring of cattle in PLF. Another study group \cite{tu2021automatic} developed the PigMS R-CNN framework \cite{huang2019mask} to enhance the monitoring of group-housed pigs. This framework employs a 101-layer residual network along with a feature pyramid network and soft non-maximum suppression to effectively detect and segment pigs, thereby improving the accuracy of identifying and locating individual pigs in complex environments.

\subsection*{Model Generalization, Pre-Training, and Fine-Tuning}

Although implementing image-based systems in livestock production has become more common, current studies primarily focus on accuracy in homogenous environments and rarely address the challenges of model generalization. How a model can generalize to new environments is critical when farm operators deploy existing CV models in their own settings. Good generalization performance ensures that the model can reproduce similar results as reported in the original study, even in new environments with different conditions. Factors such as camera angles and the presence of occlusions can impact generalization in the deployment environment. Deploying the same model in a new environment does not necessarily guarantee the same performance as reported in the original study. Li et al. \cite{li2021practices} also pointed out that the lighting conditions on farms in real applications can be highly variable, leading to poor generalization performance.

One explanation for poor generalization is the discrepancy between the pre-training process and the specific use case. Most CV models are released with pre-trained weights, obtained from training on a large-scale dataset. For example, the COCO dataset \cite{lin2014microsoft} is a general-purpose dataset containing over 200,000 images and a wide range of object categories, such as vehicles and household items. Directly deploying a model pre-trained on the COCO dataset to detect cows in a farm setting may not ensure satisfactory performance, as the dataset does not contain enough cow instances in different view angles or occlusions. To alleviate this discrepancy, fine-tuning is a common practice that modifies the prediction head of the pre-trained model and updates the weights on a new dataset more relevant to the specific use case. Most application studies have adopted this approach to improve model generalization on their specific tasks \cite{han2021pre,guirguis2022cfa,gupta2023novel}.

Nevertheless, fine-tuning is not guaranteed to be successful, as the outcome depends on both the quantity and quality of the annotated dataset. For example, Zin et al. \cite{zin_automatic_2020} deployed an object detection model to recognize cow ear tags in a dairy farm. Although the model achieved a high accuracy of 92.5\% in recognizing the digits on the ear tags, more than 10,000 images were required for fine-tuning. Assembling such a large dataset is labor-intensive and requires specific training in annotating the images. The annotated dataset is rigorously organized in a specific format. For example, the COCO annotation format \cite{lin2014microsoft} stores image information, object class, and annotations of the entire dataset in one nested JSON format. In contrast, the YOLO format \cite{ultralytics2023datasets}, another common format for object localization, stores information of one image in one text file, with each line representing one object instance in the image. Additionally, unlike the COCO format that stores bounding box coordinates in absolute pixel values, the YOLO format stores the coordinates in relative values to the image size. These technical details are key to valid annotations, which are usually facilitated by professional annotation tools such as Labelme \cite{labelme2023}, CVAT \cite{cvat2023}, or Roboflow \cite{roboflow2023}.

\subsection*{Model Complexity and Performance}

Another factor affecting model generalization is model complexity. Generally, model complexity is quantified by the number of learnable parameters in a model \cite{hu2021model}. A more complex model can often generalize better to unseen data with high accuracy. However, this high complexity also comes at the cost of computational resources in the form of memory or time \cite{justus2018predicting}. The computational cost may further limit how models can be deployed in real-world applications, where real-time processing or edge computing is desired for fast or compact systems. For instance, the VGG-16 model \cite{simonyan2014very} has 138 million parameters and requires a video memory of at least 8GB, while the ResNet-152 \cite{he2016deep} has around 60 million parameters with a recommended video memory of 11GB. Recent models for object detection, such as YOLOv8 \cite{ultralyticsYOLOv8} and YOLOv9 \cite{wang2024yolov9}, have been developed in different sizes, providing a flexible choice for researchers to balance between generalization performance and computational cost. In YOLOv8, the spectrum of model complexity ranges from the highly intricate YOLOv8x, containing 68.2 million parameters, to more streamlined variants like YOLOv8n with only 3.2 million parameters. The memory demand for the model architecture alone, without considering the intermediate results during training or inference, is larger by a factor of 21 for YOLOv8x (136.9 megabytes) compared to YOLOv8n (6.5 megabytes). Therefore, the trade-off between model complexity and computational cost is a critical factor to consider when deploying CV models in real-world scenarios.

\subsection*{YOLO Models}

Before YOLO, object detection methods typically involved either using “sliding windows with classifier” or “region proposals with classifier.” The sliding windows method required running the classifier hundreds or thousands of times per image. On the other hand, advanced region proposal-based approaches divided the task into two steps: first, identifying potential object regions (i.e., region proposals) and then applying a classifier to these regions. In contrast, YOLO models are capable of performing object detection in a single pass through the network, which is why the acronym YOLO stands for “You Only Look Once.”

YOLOv8 \cite{ultralyticsYOLOv8}, building on the YOLOv5 \cite{Jocher2020YOLOv5} architecture, incorporates the C2F module (cross-stage partial bottleneck with two convolutions), a refinement of the CSPLayer of YOLOv5 featuring two convolutional operations. It employs SiLU activation over traditional ReLU and Sigmoid \cite{elfwing2018sigmoid} for smoother gradient flow, enhancing CNN performance. The module divides input from a convolutional layer, processes one half through bottleneck layers (offering two types: with and without shortcuts similar to ResNet \cite{targ2016resnet}), then merges it back for further convolution. This design, along with a spatial pyramid pooling fast (SPPF) layer in its backbone, supports efficient feature pooling and multi-scale detection by using three distinct heads, thereby optimizing object detection across varying sizes. Furthermore, YOLOv8 innovates with an anchor-free approach, directly predicting bounding boxes and confidence scores, thus simplifying the network and reducing computational overhead \cite{law2018cornernet,duan2019centernet,tian2019fcos}.

Deep learning models, including the YOLO family, encounter an information bottleneck issue \cite{tishby2015deep,tishby2000information}, where the retention of input information diminishes as data is compressed into features. This loss is exacerbated in deeper network layers, often leading to reduced model efficacy. One approach to mitigate this involves expanding the model’s width, i.e., increasing the number of parameters, which allows for broader feature mapping and potentially recaptures lost information. However, simply increasing model size can lead to unreliable data outputs and does not proportionally enhance model performance.

YOLOv9 addresses these challenges through innovations like Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN) \cite{wang2024yolov9}. PGI optimizes gradient generation to minimize deep layer information loss, featuring a main branch for inference and auxiliary branches for enhanced training. GELAN, by integrating and pooling convolutional layers, ensures robust feature retention. This adaptive system notably boosts inference speed by 20\% \cite{wang2024yolov9} on the COCO dataset \cite{lin2014microsoft}, while its multi-level auxiliary information facilitates the detection of objects across varying sizes, making YOLOv9 particularly effective in identifying smaller objects compared to its predecessors.

\subsection*{Public Datasets}

A public dataset helps the community to develop methodology based on the same baseline. One famous example in computer vision is the ImageNet dataset \cite{deng2009imagenet}, which serves as a benchmark for image classification. AlexNet \cite{krizhevsky2017imagenet}, the winner of the ImageNet Large Scale Visual Recognition Challenge in 2012, demonstrated its outstanding capability to classify images in the ImageNet dataset using Rectified Linear Units (ReLU) as the activation function, rather than the traditional sigmoid function. The success of AlexNet accelerated the development of CV models in the following years, such as VGG \cite{karen2014very}, GoogLeNet \cite{szegedy2015going}, ResNet \cite{targ2016resnet}, and DenseNet \cite{huang2017densely}. However, similar to the challenges that pre-trained models face in specific use cases, a generic public dataset, such as ImageNet \cite{deng2009imagenet} and COCO \cite{lin2014microsoft}, may not be sufficient for PLF applications. 

There have been efforts to create public datasets for livestock scenarios. For example, the CattleEyeView dataset was collected to support applications like cattle pose estimation and behavior analysis, providing extensive annotations across 30,703 frames from top-down video sequences of cows \cite{ong2023cattleeyeview}. Another study \cite{t2020long} leverages a public dataset for pigs comprising 3600 images from 12 videos of group-housed pigs. The dataset is particularly designed for applications such as pig tracking. Additionally, the "OpenCows2020" dataset, developed by researchers from the University of Bristol, is a public dataset specifically designed for advancing non-intrusive monitoring of cattle. It supports precision farming applications such as automated productivity assessment, health and welfare monitoring, and veterinary research, including behavioral analysis and disease outbreak tracing. The dataset consists of 11,779 images with 13,026 labeled objects, mainly focusing on cattle \cite{visualization-tools-for-opencows2020-dataset}. 

\subsection*{Study Objectives}

This study aims to explore model generalization across varying environmental settings and model complexities within the context of indoor cow localization. It seeks to examine three hypotheses:

\begin{itemize}
    \item \textbf{Model generalization is equally influenced by changes in lighting conditions and camera angles.} Should camera angles prove more impactful than lighting conditions, it would be advisable to prioritize camera placement when deploying CV models in new environments.
    \item \textbf{Higher model complexity guarantees better generalization performance.} If a highly complex model does not ensure superior performance, future studies might consider adopting less computationally demanding models that still enhance performance.
    \item \textbf{The advantages of using fine-tuned models as initial training weights are persistent over pre-trained models.} If the advantages diminish as the training sample size increases in a similar cow localization task but different environments, the fine-tuning efforts may be deemed unnecessary when the deployment environment varies over multiple locations on a farm.
\end{itemize}

To facilitate these investigations, a public dataset named COws LOcalization (COLO) \cite{COLODataset2023} will be developed and made available to the community. The findings of this study are expected to provide practical guidelines for Precision Livestock Farming (PLF) researchers on deploying CV models, considering available resources and anticipated performance.