\section{Introduction}

\subsection*{Define object localization and applications}

Localizing livestock individuals from images or videos has became an essential task in precision livestock farming (PLF) \cite{fernandes2020image}. Such techniques allows researchers and farm managers to monitor the health and well-being of animals in real-time, optimizing their resource management and improving sustainability \cite{morrone2022industry} \cite{hao2023cattle}. Technically speaking, in the field of computer vision (CV), which is a subfield of artificial intelligence (AI) that focuses on translating visual information into actionable insights, the localization tasks can be further categorized into object detection, object segmentation, and pose estimation. Object detection is the simplest form of localization, which localizes objects of interest by enclosing them within a rectangular bounding box defined by x and y coordinates, pixel width, and pixel height \cite{viola2001rapid}. 


Over the years, 2D object detection methods have gradually improved as modern deep learning techniques have evolved. Some popular models for 2D object detection with bounding boxes are YOLO (You Only Look Once) \cite{redmon2016you}, Faster R-CNN (Region Convolutional Neural Networks) \cite{girshick2015fast}, and SSD (Single Shot MultiBox Detector) \cite{liu2016ssd}. These models have been adopted and applied by animal scientists for detection in precision livestock farming.

For example, in \cite{yu2022automatic}, the authors used the DRN-YOLO model \cite{xu2020improved} to predict the eating behavior of dairy cows. This approach significantly improved the accuracy and efficiency of real-time monitoring of feeding behavior, outperforming traditional methods in complex farm settings.

Another notable work is presented in \cite{nasirahmadi2019deep}, where the authors developed a posture detection system for pigs using deep learning models such as Faster R-CNN, SSD, and R-FCN, coupled with 2D imaging. This system accurately identifies standing and lying postures of pigs under commercial farm conditions.

To achieve finer localization, object segmentation is employed to outline object contours pixel-wise, while pose estimation is performed by orienting and marking the keypoints of the object \cite{hariharan2015hypercolumns}. Some popular object segmentation models include Mask R-CNN \cite{he2017mask}, MS R-CNN \cite{huang2019mask}, and U-net \cite{siddique2021u}.

This method of segmentation has also been applied in the field of precision livestock farming (PLF). In the study \cite{noe2022automatic}, the authors developed a method using Mask R-CNN \cite{he2017mask} to segment and outline cattle in feedlots. Their technique enhances images and extracts key frames to accurately detect cattle, achieving superior precision with a mean pixel accuracy of 0.92. This supports advanced, real-time monitoring of cattle in PLF.

In the study \cite{tu2021automatic}, researchers developed the PigMS R-CNN framework \cite{huang2019mask} to enhance the monitoring of group-housed pigs. This framework employs a 101-layer residual network along with a feature pyramid network and soft non-maximum suppression to effectively detect and segment pigs, thereby improving the accuracy of identifying and locating individual pigs in complex environments.







\subsection*{Model generalization, pre-training, and fine-tuning}

Although implementing image-based systems in the livestock production has shown promising results, current studies merely focus on the accuracy on homogenous environments and rarely address the challenges of model generalization. Model generalization refers to how well a model can perform on unseen data, which is crucial when ones want to reproduce existing studies or models in their own environments. The generalization of a CV model can be affected by a variety of factors in the deployment environment, such as camera angles and the presence of occlusions. Deploying the same model in a new environment with different conditions cannot necessarily guarantee the same performance as reported in the original study \cite{li2021practices} also pointed out that lightning condition of farms in real applications can be highly variable, leading a poor generalization th new environments.

One explaination for the poor generalization is the discrepancy between the pre-training process and the specific use case. Most CV models are released with pre-trained weights, which were obtained from the results of training on a large-scale dataset. For example, the COCO dataset \cite{lin2014microsoft} is a general-purpose dataset that contains more than 200 thousands images and a wide range of object categories, such as vehicles and household items. Directly deploying a model pre-trained on the COCO dataset to specifically detect cows in a farm setting may not ensure satisfactory performance, as the dataset does not contain enough cow instances in different view angles or occlusions. To alleviate the discrepancy, fine-tuning is a common practice that modifies the prediction head of the pre-trained model and updates the weights on a new dataset that is more relevant to the specific use case. Most application studies have adopted this approach to improve the model generalization on their specific tasks\cite{han2021pre,guirguis2022cfa,gupta2023novel}.

Nevertheless, the fine-tuning is not guaranteed to be successful, as the outcome depends on both the quantity and quality of the annotated dataset. For example, \cite{zin_automatic_2020} deployed an object detection model to recognize cow ear tags in a dairy farm. Although the model achieved a high accuracy of 92.5\% on recognizing the digits on the ear tags, more than 10 thousand images were required for fine-tuning the model. Assemblying such a large dataset is labor-intensive and requires specific training in annotating the images. Because the annotated dataset is rigorously organized in specific format. For example, the COCO annotation format \cite{lin2014microsoft} store the image information, object class, and annotations of the entire dataset in one nested JSON format \cite{lin2014microsoft}. Whereas the YOLO format \cite{ultralytics2023datasets}, another common format for object localization, stores information of one image in one text file, with each line representing one object instance in the image. Additionally, unlike the COCO format that stores bounding box coordinates in absolute pixel values, the YOLO format stores the coordinates in relative values to the image size. These technical details are keys to valid annotations, which are usually helped by the professional annotation tools such as labelme \cite{labelme2023}, CVAT \cite{cvat2023}, or Roboflow \cite{roboflow2023}.

\subsection*{Model Complexity and Performance}

Another perspective that affects the model generalization is model complexity. In general, model complexity is quantified by the number of learnable parameters in a model \cite{hu2021model}. A more complex model often can better generalize to unseen data with higy accuracy. However, such high complexity also comes with a cost of computational resources in a form of either memory or time \cite{justus2018predicting}. The computational cost may further limit how the models can be deployed in real-world applications, where real-time processing or edge computing is desired for fast or compact systems. For instance, the VGG-16 model \cite{simonyan2014very} has 138 million parameters and recommends a video memory of at least 8GB, while the ResNet-152 \cite{he2016deep} has around 60 million parameters with a recommended video memory of 11GB. Additionally, recent models for object detection such as YOLOv8 \cite{ultralyticsYOLOv8} and YOLOv9 \cite{wang2024yolov9} have been developed in different sizes and therefore provide a flexible choice for researchers to balance between the generalization performance and the computational cost. In YOLOv8, the spectrum of model complexity ranges from the highly intricate, such as YOLOv8x containing 68.2 million parameters, to more streamlined variants YOLOv8n with only 3.2 million parameters. And the demand for the memory, solely from the model architecture without considering the intermediate results during the training or inferenecne process, is larger in a factor of 21 for YOLOv8x (136.9 megabytes) compared to YOLOv8n (6.5 megabytes). Therefore, the trade-off between the model complexity and the computational cost is a critical factor to consider in deploying CV models in real-world scenarios.

\subsection*{YOLO Models}
Before YOLO object detection methods typically involved either using "sliding windows with classifier" or "region proposals, with classifier." The sliding windows method required to run the hundreds or thousands of times, per image. On the hand advanced region proposals based approaches divided the task into two steps; first identifying potential object regions (region proposal) and then applying a classifier to these regions. In contrast YOLO models are capable of performing object detection in a pass through the network, which's why acronym YOLO stands for "You Only Look Once."

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



YOLOv8 \cite{ultralyticsYOLOv8}, building on the YOLOv5 \cite{Jocher2020YOLOv5} architecture, incorporates the C2F module (cross-stage partial bottleneck with two convolutions) a refinement of the CSPLayer of YOLOv5 featuring two convolutional operations. It employs SiLU activation over traditional ReLU and Sigmoid \cite{elfwing2018sigmoid} for smoother gradient flow, enhancing CNN performance. The module divides input from a convolutional layer, processes one half through bottleneck layers (offering two types: with and without shortcuts similar to ResNet \cite{targ2016resnet}), then merges it back for further convolution. This design, along with a spatial pyramid pooling fast (SPPF) layer in its backbone, supports efficient feature pooling and multi-scale detection by using three distinct heads, thereby optimizing object detection across varying sizes. Furthermore, YOLOv8 innovates with an anchor-free approach, directly predicting bounding boxes and confidence scores, thus simplifying the network and reducing computational overhead \cite{law2018cornernet,duan2019centernet,tian2019fcos}.

Deep learning models, including the YOLO family, encounter an information bottleneck issue \cite{tishby2015deep,tishby2000information}, where the retention of input information diminishes as data is compressed into features. This loss is exacerbated in deeper network layers, often leading to reduced model efficacy. One approach to mitigate this involves expanding the model's width, i.e., increasing the number of parameters, which allows for broader feature mapping and potentially recaptures lost information. However, simply increasing model size can lead to unreliable data outputs and does not proportionally enhance model performance.

YOLOv9 addresses these challenges through innovations like Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN) \cite{wang2024yolov9}. PGI optimizes gradient generation to minimize deep layer information loss, featuring a main branch for inference and auxiliary branches for enhanced training. GELAN, by integrating and pooling convolutional layers, ensures robust feature retention. This adaptive system notably boosts inference speed by 20\% \cite{wang2024yolov9}on the COCO dataset \cite{lin2014microsoft}, while its multi-level auxiliary information facilitates the detection of objects across varying sizes, making YOLOv9 particularly effective in identifying smaller objects compared to its predecessors.








\subsection*{Public Datasets}

A public dataset helps the community to develop methodology based on the same baseline. One famous example in computer vision is the ImageNet dataset \cite{deng2009imagenet}, which serves as a benchmark for image classification. AlexNet \cite{krizhevsky2017imagenet}, the winner of the ImageNet Large Scale Visual Recognition Challenge in 2012, show its outstanding capability to classify images in ImageNet dataset using Rectified Linear Units (ReLU) as the activation function than the traditional sigmoid function. The success of AlexNet accelerate the developement of CV models in the following years, such as VGG \cite{karen2014very}, GoogLeNet \cite{szegedy2015going}, ResNet \cite{targ2016resnet}, and DenseNet \cite{huang2017densely}. However, similar to the challenges that pre-trained models face in the specific use case, a generic public dataset, such as ImageNet \cite{deng2009imagenet}and COCO \cite{lin2014microsoft}, may not be sufficient to PLF applications. There  were efforts to create public datasets for livestock scenario.For example, the CattleEyeView dataset was collected to support applications like cattle pose estimation and behavior analysis, providing extensive annotations across 30,703 frames from top-down video sequences of cows \cite{ong2023cattleeyeview}.
Another study \cite{t2020long} leverages a public dataset for pigs comprising 3600 images from 12 videos of group-housed pigs. The dataset is particularly designed for applications such as pig tracking. Also the "OpenCows2020" dataset, developed by researchers from the University of Bristol, is a public dataset specifically designed for advancing non-intrusive monitoring of cattle. It supports precision farming applications such as automated productivity assessment, health and welfare monitoring, and veterinary research including behavioral analysis and disease outbreak tracing. The dataset consists of 11,779 images with 13,026 labeled objects, mainly focusing on cattle\cite{visualization-tools-for-opencows2020-dataset}.
Another work \cite{ye2022superanimal} describes the dataset as publicly available. They collected datasets from the community and contributed two new datasets for model building, namely \textit{iRodent} and \textit{MausHaus}. These datasets are detailed in the "Datasheets" section of the paper.

\subsection*{Study Objectives}

This study aims to explore model generalization across varying environmental settings and model complexities within the context of indoor cow localization. It seeks to exmaine three practical hypotheses:

\begin{itemize}
    \item \textbf{Model generalization is equally influenced by changes in lighting conditions and camera angles.} Should camera angles prove more impactful than lighting conditions, it would be advisable to prioritize camera placement when deploying computer vision (CV) models in new environments.
    \item \textbf{There is a positive correlation between model complexity and generalization performance.} If a highly complex model does not ensure superior performance, future studies might consider adopting less computationally demanding models that still enhance performance.
    \item \textbf{The advantages of using fine-tuned models as initial training weights are persistent over pre-trained models.} If the advantages are disminished as the training sample size increasese in a similar cow localization task but different environments, the fine-tuning efforts may be deemed unnecessary when the deployment environment varies over multiple locations on a farm.
\end{itemize}

To facilitate these investigations, a public dataset named COws LOcalization (COLO) \cite{COLODataset2023} will be developed and made available to the community. The findings of this study are expected to provide practical guidelines for Precision Livestock Farming (PLF) researchers on deploying CV models, considering available resources and anticipated performance.
