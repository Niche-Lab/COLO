\section{Discussion}


\subsection{Potential causes for the performance drop in camera view points}

Figure ~\ref{fig:model-performance} provides a comparative analysis of model behavior under various data configurations. It is clear that the `b\_light' configuration shown a much better performance relative to the heterogeneous viewpoint-oriented configurations `a1\_t2s' and `a2\_s2t'.

Despite the various challenges in adapting models from day to night conditions, the `b\_light' configuration consistently maintains high precision, closely mirroring the `0\_all' configuration across all training sample sizes. This suggests that changes in lighting have less impact on the models ability to detect objects compared to changes in viewpoint. This robustness to lighting could be attributed to the inclusion of diverse lighting conditions in the training phase, specifically, model performance benefited from pixel-wise augmentation techniques such as adjustments to hue, saturation, and value(HSV). These augmentations introduced a variety of color variations to the images, enhancing the model's ability to generalize across different visual conditions.Such methods adjust the lighting and contrast in training images, enhancing the model's ability to generalize to different illumination scenarios. Moreover, these YOLO models benefit from pre-training on the COCO dataset, which is characterized by a wide array of images with varied lighting, aiding their adaptability to shifts in light.

On the other hand, the models perform suboptimally in scenarios involving changes in viewpoint. This is because each new viewpoint can introduce fundamentally different object features, which are not replicated through standard data augmentation methods like lighting or affine transformations. For example, when the camera was placed at a lower angle, cows were occluded more by stalls and fences. These additional objects posed object-wise variation that can not be overcome by augmentation of HSV space or image translation.

The analysis of Figure ~\ref{fig:finetune vs raw model} provides another insight on the performance across homogeneous viewpoint data configurations, specifically `1\_top' and `2\_side'. The data demonstrates that the `1\_top' configuration consistently yields higher mAP values regardless of the training sample size and weight initialization conditions. This implies that the `2\_side' configuration, where both training and test images are captured from the side view, presents a more formidable challenge for cow detection compared to the `1\_top' configuration.

Similarly to the discussion of the difference between lighting and viewpoint in the images, the side view poses difficulties due to occlusions by neighboring cows and additional distractions, such as obstacles in aisles and fences. Furthermore, cows located further away in side-view images may not be as visible, complicating feature extraction. In contrast, the `1\_top' configuration benefits from an unobstructed aerial perspective, ensuring that the top view of all cow instances is clearly visible and free from such obstructions. This distinction in visibility between the two configurations contributes to the ease of feature extraction and ultimately, the performance disparity observed.

\subsection{The choice of model complexity is task-specific}

The results from the Figure ~\ref{fig:model complexcity} shows the relationship between the number of model parameters, and object detection performance across various data configurations for YOLOv8 and YOLOv9 models. 

When trained on the COCO dataset \cite{lin2014microsoft}, both YOLOv8 and YOLOv9 models exhibit an increase in mAP50 as the number of parameters is increased. The rate of performance increase demonstrates a concave upward trajectory in the curve plotting mAP against the number of parameters \cite{ultralyticsYOLOv8,wang2024yolov9}. Despite this observation, the data from Figure ~\ref{fig:model complexcity} do not support a definitive conclusion that a higher number of parameters consistently enhances model performance. For all data configurations analyzed, performance changes minimally with parameter variations and does not adhere to a consistent pattern. Within the YOLOv8 series, the YOLOv8m model delivers optimal performance across almost all data configurations, with the exception of `a1\_t2s'. In this particular configuration, there is a gradual decline in performance as the number of parameters increases. On the other hand, the YOLOv9 series shows a slight improvement in performance for the `0\_all' and `b\_light' data configurations, while the performance for `a1\_t2s' and `a2\_s2t' configurations marginally deteriorates. Also, "a1\_t2s" is considered to be the most difficult task among all dataset according to the generalization performance observed in this study. An opposite trend that V9 is not a better model was also observed in this dataset. This outcome might suggest that when the task is too difficult, the superior capability of the model may no longer hold in such case. In general, the performance trends of these models with respect to parameter increments are not uniform; in some cases, there is a slight enhancement, whereas in others, a small reduction in performance is noted.

The prior work's findings were based on the COCO dataset, which encompasses 80 classes and predominantly features standalone images. In contrast, our study utilizes an indoor farm dataset focused exclusively on a single class: cows. Consequently, the model may not require as many parameters to optimally discern the necessary features for cow detection. This reminds the research to not only deploy one model even it is known for the best performance on a public dataset. The generalization performance maybe case-specific.

\subsection{Does custom weight initialization for model finetuning always necessary?}

In Section ~\ref{sec:pre-fine}, we examine the impact of different weight initialization strategies on the fine-tuning of models, as detailed in Figure ~\ref{fig:finetune vs raw model}. Our findings indicate that for YOLO models with fewer parameters, such as YOLOv8n and YOLOv8m, the choice of weight initialization does not make a significant different in fine-tuning performance. In contrast, larger models like YOLOv8x, YOLOv9c, and YOLOv9e exhibit improved performance when weights are initialized from a model that has been previously fine-tuned, as highlighted in cases 2 and 4 in Table ~\ref{tab:configuration}. Therefore, when fine-tuning larger models with a limited dataset, it is beneficial to utilize weights previously fine-tuned on various data configurations. Conversely, for smaller models, the weight initialization strategy does not significantly impact fine-tuning performance.



%\subsection{The Significance of Model Generalization Studies for Real-World Applications}

%Our model generalization has taken into account a variety of data configurations, encompassing lighting data configuration (`b\_light'), homogeneous (`1\_top', `2\_side'), and heterogeneous (`a1\_t2s', `a2\_s2t') viewpoint data configuration, along with a composite configuration (`0\_all'). Additionally, we've applied YOLOv8 and YOLOv9 models with varying parameter numbers to these datasets. Such a comprehensive study of model generalization is involved in selecting an optimal model by considering relevant factors. For instance, for a farm with cameras installed overhead, a model fine-tuned with the `1\_top' data configuration would likely be most effective. Furthermore, by analysing additional results like computation time and training sample size, the best-suited model for the scenario can be identified.

%In practical applications, specifically within the context of an indoor farm, this generalization study is helpful for choosing the most appropriate model to enhance the accuracy of cow detection. It not only guides the selection process but also ensures that the chosen model is tailored to the specificities of the environment.



%\subsection{DETR}

% \subsection{How does the model generalization study help in real-world applications?}






% %Explain the interaction between YOLOv8 and v9: when increasing the number of parameters in V8, the performance is not always improved. Except for the dataset "a1\_t2s",  the peak performance was observed on YOLOv8m which has only 23 millions of parameters. However, In V9, an improvement can be expected even when the parameters are increased to more than 50 million (i.e., YOLOv9e). This could be explained by the fundamental difference of the model architecture.

% %Also, "a1\_t2s" is considered to be the most difficult task among all dataset according to the generalization performance observed in this study. An opposite trend that V9 is not a better model was also observed in this dataset. This outcome might suggest that when the task is too difficult, the superior capability of the model may no longer hold in such case. This reminds the research to not only deploy one model even it is known for the best performance on a public dataset. The generalization performance maybe case-specific.

% %%%%How does the model generalization study help in real-world applications?