
@article{rajput_evaluation_2023,
	title = {Evaluation of a decided sample size in machine learning applications},
	volume = {24},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-023-05156-9},
	doi = {10.1186/s12859-023-05156-9},
	abstract = {An appropriate sample size is essential for obtaining a precise and reliable outcome of a study. In machine learning (ML), studies with inadequate samples suffer from overfitting of data and have a lower probability of producing true effects, while the increment in sample size increases the accuracy of prediction but may not cause a significant change after a certain sample size. Existing statistical approaches using standardized mean difference, effect size, and statistical power for determining sample size are potentially biased due to miscalculations or lack of experimental details. This study aims to design criteria for evaluating sample size in ML studies. We examined the average and grand effect sizes and the performance of five ML methods using simulated datasets and three real datasets to derive the criteria for sample size. We systematically increase the sample size, starting from 16, by randomly sampling and examine the impact of sample size on classifiers’ performance and both effect sizes. Tenfold cross-validation was used to quantify the accuracy.},
	number = {1},
	urldate = {2024-03-06},
	journal = {BMC Bioinformatics},
	author = {Rajput, Daniyal and Wang, Wei-Jen and Chen, Chun-Chuan},
	month = feb,
	year = {2023},
	keywords = {Machine learning, Criteria, Effect sizes, Sample size},
	pages = {48},
	file = {Full Text PDF:/Users/niche/Zotero/storage/9DX5T6QW/Rajput et al. - 2023 - Evaluation of a decided sample size in machine lea.pdf:application/pdf;Snapshot:/Users/niche/Zotero/storage/HU4SCBVR/s12859-023-05156-9.html:text/html},
}

@article{hussain_yolo-v1_2023,
	title = {{YOLO}-v1 to {YOLO}-v8, the {Rise} of {YOLO} and {Its} {Complementary} {Nature} toward {Digital} {Manufacturing} and {Industrial} {Defect} {Detection}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/11/7/677},
	doi = {10.3390/machines11070677},
	abstract = {Since its inception in 2015, the YOLO (You Only Look Once) variant of object detectors has rapidly grown, with the latest release of YOLO-v8 in January 2023. YOLO variants are underpinned by the principle of real-time and high-classification performance, based on limited but efficient computational parameters. This principle has been found within the DNA of all YOLO variants with increasing intensity, as the variants evolve addressing the requirements of automated quality inspection within the industrial surface defect detection domain, such as the need for fast detection, high accuracy, and deployment onto constrained edge devices. This paper is the first to provide an in-depth review of the YOLO evolution from the original YOLO to the recent release (YOLO-v8) from the perspective of industrial manufacturing. The review explores the key architectural advancements proposed at each iteration, followed by examples of industrial deployment for surface defect detection endorsing its compatibility with industrial requirements.},
	language = {en},
	number = {7},
	urldate = {2024-03-15},
	journal = {Machines},
	author = {Hussain, Muhammad},
	month = jul,
	year = {2023},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {object detection, industrial defect detection, quality inspection, smart manufacturing},
	pages = {677},
	file = {Full Text PDF:/Users/niche/Zotero/storage/R3GQQIP3/Hussain - 2023 - YOLO-v1 to YOLO-v8, the Rise of YOLO and Its Compl.pdf:application/pdf},
}

@misc{noauthor_make_nodate,
	title = {{MAKE} {\textbar} {Free} {Full}-{Text} {\textbar} {A} {Comprehensive} {Review} of {YOLO} {Architectures} in {Computer} {Vision}: {From} {YOLOv1} to {YOLOv8} and {YOLO}-{NAS}},
	url = {https://www.mdpi.com/2504-4990/5/4/83},
	urldate = {2024-03-15},
}

@article{terven_comprehensive_2023,
	title = {A {Comprehensive} {Review} of {YOLO} {Architectures} in {Computer} {Vision}: {From} {YOLOv1} to {YOLOv8} and {YOLO}-{NAS}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-4990},
	shorttitle = {A {Comprehensive} {Review} of {YOLO} {Architectures} in {Computer} {Vision}},
	url = {https://www.mdpi.com/2504-4990/5/4/83},
	doi = {10.3390/make5040083},
	abstract = {YOLO has become a central real-time object detection system for robotics, driverless cars, and video monitoring applications. We present a comprehensive analysis of YOLO’s evolution, examining the innovations and contributions in each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with transformers. We start by describing the standard metrics and postprocessing; then, we discuss the major changes in network architecture and training tricks for each model. Finally, we summarize the essential lessons from YOLO’s development and provide a perspective on its future, highlighting potential research directions to enhance real-time object detection systems.},
	language = {en},
	number = {4},
	urldate = {2024-03-15},
	journal = {Machine Learning and Knowledge Extraction},
	author = {Terven, Juan and Córdova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro},
	month = dec,
	year = {2023},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep learning, computer vision, object detection, YOLO},
	pages = {1680--1716},
	file = {Full Text PDF:/Users/niche/Zotero/storage/D7YUF99U/Terven et al. - 2023 - A Comprehensive Review of YOLO Architectures in Co.pdf:application/pdf},
}

@article{li_practices_2021,
	title = {Practices and {Applications} of {Convolutional} {Neural} {Network}-{Based} {Computer} {Vision} {Systems} in {Animal} {Farming}: {A} {Review}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	shorttitle = {Practices and {Applications} of {Convolutional} {Neural} {Network}-{Based} {Computer} {Vision} {Systems} in {Animal} {Farming}},
	url = {https://www.mdpi.com/1424-8220/21/4/1492},
	doi = {10.3390/s21041492},
	abstract = {Convolutional neural network (CNN)-based computer vision systems have been increasingly applied in animal farming to improve animal management, but current knowledge, practices, limitations, and solutions of the applications remain to be expanded and explored. The objective of this study is to systematically review applications of CNN-based computer vision systems on animal farming in terms of the five deep learning computer vision tasks: image classification, object detection, semantic/instance segmentation, pose estimation, and tracking. Cattle, sheep/goats, pigs, and poultry were the major farm animal species of concern. In this research, preparations for system development, including camera settings, inclusion of variations for data recordings, choices of graphics processing units, image preprocessing, and data labeling were summarized. CNN architectures were reviewed based on the computer vision tasks in animal farming. Strategies of algorithm development included distribution of development data, data augmentation, hyperparameter tuning, and selection of evaluation metrics. Judgment of model performance and performance based on architectures were discussed. Besides practices in optimizing CNN-based computer vision systems, system applications were also organized based on year, country, animal species, and purposes. Finally, recommendations on future research were provided to develop and improve CNN-based computer vision systems for improved welfare, environment, engineering, genetics, and management of farm animals.},
	language = {en},
	number = {4},
	urldate = {2024-04-04},
	journal = {Sensors},
	author = {Li, Guoming and Huang, Yanbo and Chen, Zhiqian and Chesser, Gary D. and Purswell, Joseph L. and Linhoss, John and Zhao, Yang},
	month = jan,
	year = {2021},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {animal farming, computer vision system, convolutional neural network, deep learning},
	pages = {1492},
	file = {Full Text PDF:/Users/niche/Zotero/storage/CNY6MSQK/Li et al. - 2021 - Practices and Applications of Convolutional Neural.pdf:application/pdf},
}

@article{zin_automatic_2020,
	title = {Automatic {Cow} {Location} {Tracking} {System} {Using} {Ear} {Tag} {Visual} {Analysis}},
	volume = {20},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/20/12/3564},
	doi = {10.3390/s20123564},
	abstract = {Nowadays, for numerous reasons, smart farming systems focus on the use of image processing technologies and 5G communications. In this paper, we propose a tracking system for individual cows using an ear tag visual analysis. By using ear tags, the farmers can track specific data for individual cows such as body condition score, genetic abnormalities, etc. Specifically, a four-digit identification number is used, so that a farm can accommodate up to 9999 cows. In our proposed system, we develop an individual cow tracker to provide effective management with real-time upgrading enforcement. For this purpose, head detection is first carried out to determine the cow’s position in its related camera view. The head detection process incorporates an object detector called You Only Look Once (YOLO) and is then followed by ear tag detection. The steps involved in ear tag recognition are (1) finding the four-digit area, (2) digit segmentation using an image processing technique, and (3) ear tag recognition using a convolutional neural network (CNN) classifier. Finally, a location searching system for an individual cow is established by entering the ID numbers through the application’s user interface. The proposed searching system was confirmed by performing real-time experiments at a feeding station on a farm at Hokkaido prefecture, Japan. In combination with our decision-making process, the proposed system achieved an accuracy of 100\% for head detection, and 92.5\% for ear tag digit recognition. The results of using our system are very promising in terms of effectiveness.},
	language = {en},
	number = {12},
	urldate = {2024-04-05},
	journal = {Sensors},
	author = {Zin, Thi Thi and Pwint, Moe Zet and Seint, Pann Thinzar and Thant, Shin and Misawa, Shuhei and Sumi, Kosuke and Yoshida, Kyohiro},
	month = jan,
	year = {2020},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {convolutional neural network, digit segmentation, ear tag recognition, location searching, object detector},
	pages = {3564},
	file = {Full Text PDF:/Users/niche/Zotero/storage/DRFPLKY9/Zin et al. - 2020 - Automatic Cow Location Tracking System Using Ear T.pdf:application/pdf},
}

@misc{greif_dgreifring_2024,
	title = {dgreif/ring},
	copyright = {MIT},
	url = {https://github.com/dgreif/ring},
	abstract = {Unofficial packages for Ring Doorbells, Cameras, Alarm System, and Smart Lighting},
	urldate = {2024-04-23},
	author = {Greif, Dusty},
	month = apr,
	year = {2024},
	note = {original-date: 2018-10-12T22:53:01Z},
	keywords = {alarm, homebridge, homebridge-plugin, homekit, ring},
}
